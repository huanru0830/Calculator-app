{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aUB4xrFdLkr8"
      },
      "outputs": [],
      "source": [
        "restart = True\n",
        "epoch_to_pickup = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XtiXE04uGB_U"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "\n",
        "from tensorflow.keras.layers import StringLookup\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import random\n",
        "import contextlib\n",
        "import io\n",
        "import re\n",
        "import string\n",
        "import gc  # Import the garbage collector module\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import TextVectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUgiww4oQ75T",
        "outputId": "3809d190-1a4b-4dcc-c00f-3e6bdde74610"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "irakMtGnaImf"
      },
      "outputs": [],
      "source": [
        "path = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "nDl6_okDOUyY"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# path = '/content/drive/My Drive/M6_Fall2023e/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv4r-dKnSRKz"
      },
      "source": [
        "## Functions for downloading text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xzLUaBa2Xmnb"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "\n",
        "    text = text.replace(\"Project Gutenberg\", \"\")\n",
        "    text = text.replace(\"Gutenberg\", \"\")\n",
        "\n",
        "    # Remove carriage returns\n",
        "    text = text.replace(\"\\r\", \"\")\n",
        "\n",
        "    # fix quotes\n",
        "    text = text.replace(\"“\", \"\\\"\")\n",
        "    text = text.replace(\"”\", \"\\\"\")\n",
        "\n",
        "    # Replace any capital letter at the start of a word with ^ followed by the lowercase letter\n",
        "    text = re.sub(r\"(?<![a-zA-Z])([A-Z])\", lambda match: f\"^{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Replace all other capital letters with lowercase\n",
        "    text = re.sub(r\"([A-Z])\", lambda match: f\"{match.group(0).lower()}\", text)\n",
        "\n",
        "    # Remove duplicate whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = re.sub(r\"\\n+\", \"\\n\", text)\n",
        "    text = re.sub(r\"\\t+\", \"\\t\", text)\n",
        "\n",
        "    # Replace whitespace characters with special words\n",
        "    text = re.sub(r\"(\\t)\", r\" zztabzz \", text)\n",
        "    text = re.sub(r\"(\\n)\", r\" zznewlinezz \", text)\n",
        "    text = re.sub(r\"(\\s)\", r\" zzspacezz \", text)\n",
        "\n",
        "    # Split before and after punctuation\n",
        "    for punctuation in string.punctuation:\n",
        "        text = text.replace(punctuation, f\" {punctuation} \")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nFKehxVF9AxD"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(text):\n",
        "\n",
        "    # Replace special words with whitespace characters\n",
        "    text = text.replace(\"zztabzz\", \"\\t\")\n",
        "    text = text.replace(\"zznewlinezz\", \"\\n\")\n",
        "    text = text.replace(\"zzspacezz\", \" \")\n",
        "\n",
        "    # Remake capital letters at beginning of words\n",
        "    text = re.sub(r\"\\^([a-z])\", lambda match: f\"{match.group(1).upper()}\", text)\n",
        "\n",
        "    text = text.replace(\"^\", \"\")\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rtd9QyvUWqzi"
      },
      "outputs": [],
      "source": [
        "# def getMyText():\n",
        "#   path_to_file = tf.keras.utils.get_file('austen.txt', 'https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/austen/austen.txt')\n",
        "\n",
        "#   text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "#   # path_to_file = tf.keras.utils.get_file('903-0.txt', 'https://www.gutenberg.org/files/903/903-0.txt')\n",
        "#   # author_text += open(path_to_file, 'rb').read().decode(encoding='utf-8')[2999:-19194]\n",
        "#   # tf.io.gfile.remove(path_to_file)\n",
        "\n",
        "#   return preprocess_text(text)\n",
        "\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "def getMyText():\n",
        "    file_name = 'CharlottesWebtxt.txt'\n",
        "    file_url = 'https://raw.githubusercontent.com/ryjason/CalculatorApp/b07f97e917ed021608a2e1379af0ba0086add31c/notebooks/CharlottesWebtxt.txt'\n",
        "    local_dir = 'saved_files'  # Directory to save the file\n",
        "    local_path = os.path.join(local_dir, file_name)\n",
        "\n",
        "    try:\n",
        "        # Ensure the directory exists\n",
        "        if not os.path.exists(local_dir):\n",
        "            os.makedirs(local_dir)\n",
        "\n",
        "        # Check if the file exists locally\n",
        "        if os.path.exists(local_path):\n",
        "            print(f\"File '{file_name}' found locally. Using it.\")\n",
        "        else:\n",
        "            print(f\"File '{file_name}' not found locally. Downloading it.\")\n",
        "            # Download the file\n",
        "            downloaded_path = tf.keras.utils.get_file(file_name, file_url)\n",
        "\n",
        "            # Save the downloaded file to the designated local directory\n",
        "            with open(downloaded_path, 'rb') as source_file:\n",
        "                with open(local_path, 'wb') as dest_file:\n",
        "                    dest_file.write(source_file.read())\n",
        "\n",
        "        # Read the file's contents\n",
        "        with open(local_path, 'rb') as file:\n",
        "            text = file.read().decode(encoding='utf-8')\n",
        "\n",
        "        return preprocess_text(text)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "getMyText()\n",
        "len(getMyText())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rofy7hJ1iHVm",
        "outputId": "d36849dd-6768-4925-c3c3-c5019d909964"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'CharlottesWebtxt.txt' not found locally. Downloading it.\n",
            "File 'CharlottesWebtxt.txt' found locally. Using it.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1794534"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gsCd-ihOU02C"
      },
      "outputs": [],
      "source": [
        "def getRandomText(numbooks = 1, verbose=False):\n",
        "  download_log = io.StringIO()\n",
        "  text_random = ''\n",
        "  for b in range(numbooks):\n",
        "    foundbook = False\n",
        "    while(foundbook == False):\n",
        "      booknum = random.randint(100,60000)\n",
        "      if verbose:\n",
        "        print('Trying Book #: ',booknum)\n",
        "      if random.random() > 0.5:\n",
        "        url = 'https://www.gutenberg.org/files/' + str(booknum) + '/' + str(booknum) + '-0.txt'\n",
        "        filename_temp = str(booknum) + '-0.txt'\n",
        "      else:\n",
        "        url = 'https://www.gutenberg.org/cache/epub/' + str(booknum) + '/pg' + str(booknum) + '.txt'\n",
        "        filename_temp = 'pg' + str(booknum) + '.txt'\n",
        "      if verbose:\n",
        "        print('Trying: ', url)\n",
        "      try:\n",
        "        if verbose:\n",
        "          path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        else:\n",
        "          with contextlib.redirect_stdout(download_log):\n",
        "            path_to_file_temp = tf.keras.utils.get_file(filename_temp, url)\n",
        "        temptext = open(path_to_file_temp, 'rb').read().decode(encoding='utf-8')\n",
        "        tf.io.gfile.remove(path_to_file_temp)\n",
        "        if (temptext.find('Language: English') >= 0):\n",
        "          offset = random.randint(-20,20)\n",
        "          header = 2000\n",
        "          total_length = 200000\n",
        "          chopoffend = 10000\n",
        "          if len(temptext) > (header+total_length+offset+chopoffend):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header+offset:header+total_length+offset]\n",
        "            #print(\"Yes: \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          elif len(temptext) > (header+12000):\n",
        "            foundbook = True\n",
        "            text_random += temptext[header:-chopoffend]\n",
        "            #print(\"Yes (smaller): \" + str(booknum))\n",
        "            if verbose:\n",
        "              print('New size of dataset: ', len(text_random))\n",
        "          else:\n",
        "            if verbose:\n",
        "              print('Not long enough. Trying again...')\n",
        "            #print(\"No: \" + str(booknum) + \" too short\")\n",
        "        else:\n",
        "          if verbose:\n",
        "            print('Not English. Trying again...')\n",
        "          #print(\"No: \" + str(booknum) + \" not English\")\n",
        "        del temptext\n",
        "      except:\n",
        "        if verbose:\n",
        "          print('Not valid file. Trying again...')\n",
        "        #print(\"No: \" + str(booknum) + \" not valid\")\n",
        "        foundbook = False\n",
        "    if verbose:\n",
        "      print(\"Found \" + str(b+1) + \" books so far...\")\n",
        "  del download_log\n",
        "  #text_random = \"\".join(c for c in text_random if c in vocab)\n",
        "  #all_ids_random = ids_from_chars(tf.strings.unicode_split(text_random, 'UTF-8'))\n",
        "  #ids_dataset_random = tf.data.Dataset.from_tensor_slices(all_ids_random)\n",
        "  #sequences_random = ids_dataset_random.batch(seq_length+1, drop_remainder=True)\n",
        "  #dataset_random = sequences_random.map(split_input_target)\n",
        "  #dataset_random = (dataset_random.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  #return dataset_random\n",
        "  return preprocess_text(text_random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjEF0LKxhljS",
        "outputId": "9f6d29e4-ddbb-479d-c44f-a613dfe60335"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'CharlottesWebtxt.txt' found locally. Using it.\n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  vocab_text = getMyText()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpFvtyF_g3jY"
      },
      "source": [
        "Make vocabulary (Adapted from TensorFlow word embedding tutorial)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F8E6Q6dkMEpd"
      },
      "outputs": [],
      "source": [
        "# Vocabulary size and number of words in a sequence.\n",
        "vocab_size = 1794534\n",
        "sequence_length = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AWXUqLQ6g3KB"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Use the text vectorization layer to normalize, split, and map strings to\n",
        "  # integers. Note that the layer uses the custom standardization defined above.\n",
        "  # Set maximum_sequence length as all samples are not of the same length.\n",
        "  vectorize_layer = TextVectorization(\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zJfr5w1bTWiJ"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  # Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
        "  vectorize_layer.adapt([vocab_text])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "PmaoiyvF1Ilm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a94cf3c-dbec-4657-fa20-61c4fd88eb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8229\n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  vocabulary = vectorize_layer.get_vocabulary()\n",
        "vocab_size = len(vocabulary)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7ULNtM_8nYn"
      },
      "source": [
        "Save Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G1hjxv447INt"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  with open(path + \"vocabulary.txt\", \"w\") as file:\n",
        "    for word in vocabulary:\n",
        "        file.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7qn5MjC8p0_"
      },
      "source": [
        "Load Saved Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TLbSoqUP8Pxu"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  with open(path + \"vocabulary.txt\", \"r\") as file:\n",
        "      vocabulary = [word.strip() for word in file.readlines()]\n",
        "      vocabulary = vocabulary\n",
        "\n",
        "  vectorize_layer = TextVectorization(\n",
        "      vocabulary=vocabulary,\n",
        "      standardize='lower',\n",
        "      split='whitespace',\n",
        "      max_tokens=vocab_size,\n",
        "      output_mode='int',\n",
        "      #output_sequence_length=sequence_length\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FidGlurb1iD3",
        "outputId": "a9095ef0-abb3-4cfa-f6b6-0cc0fd489535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '[UNK]', np.str_('zzspacezz'), np.str_('\"'), np.str_('-'), np.str_(','), np.str_('='), np.str_('.'), np.str_('>'), np.str_('<'), np.str_('^'), np.str_('div'), np.str_('/'), np.str_(']'), np.str_('['), np.str_('react'), np.str_('line'), np.str_(':'), np.str_('code'), np.str_('data')]\n",
            "[np.str_('02l'), np.str_('026h'), np.str_('026c'), np.str_('022a'), np.str_('0210be90f4d3'), np.str_('01ff'), np.str_('01e85cd1be94'), np.str_('01a7'), np.str_('019'), np.str_('009'), np.str_('008'), np.str_('007a'), np.str_('006l'), np.str_('006c0'), np.str_('003h'), np.str_('003a1'), np.str_('002l'), np.str_('001h10'), np.str_('000000'), np.str_('000')]\n"
          ]
        }
      ],
      "source": [
        "print(vocabulary[:20])\n",
        "print(vocabulary[-20:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LovypAGk91Yp"
      },
      "source": [
        "Turn text into a dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Mnp0huUX93Wi"
      },
      "outputs": [],
      "source": [
        "# This function will generate our sequence pairs:\n",
        "def split_input_target(sequence):\n",
        "    input_ids = sequence[:-1]\n",
        "    target_ids = sequence[1:]\n",
        "    return input_ids, target_ids\n",
        "\n",
        "# This function will create the dataset\n",
        "def text_to_dataset(text):\n",
        "  all_ids = vectorize_layer(text)\n",
        "  ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "  del all_ids\n",
        "  sequences = ids_dataset.batch(sequence_length+1, drop_remainder=True)\n",
        "  del ids_dataset\n",
        "\n",
        "  # Call the function for every sequence in our list to create a new dataset\n",
        "  # of input->target pairs\n",
        "  dataset = sequences.map(split_input_target)\n",
        "  del sequences\n",
        "\n",
        "  # shuffle\n",
        "\n",
        "\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afRybxef_QHi"
      },
      "source": [
        "Test on vocab text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0tBa6ttN_Ufz"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = text_to_dataset(vocab_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "vq191mRgWv2w"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  text = ''.join([vocabulary[index] for index in ids])\n",
        "  return postprocess_text(text)\n",
        "\n",
        "vocabulary_adjusted = vocabulary\n",
        "vocabulary_adjusted[0] = '[UNK]'\n",
        "vocabulary_adjusted[1] = ''\n",
        "\n",
        "words_from_ids = tf.keras.layers.StringLookup(vocabulary=vocabulary_adjusted, invert=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDqaTHXFAEBD",
        "outputId": "177d14f8-1951-425f-be8d-2c3fe6e63557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: \n",
            "tf.Tensor(\n",
            "[   2    9   55   10 7295    2   45    8    2    9   45    2 4072    6\n",
            "    3 1932    3    2   19    4  139    4 1895    6    3   40    3    2\n",
            "   19    4  532    4  870    6    3  532    3    2   19    4  457    4\n",
            "  870    6    3  457    3    2   19    4 3180    4 7745    4 4112    6\n",
            "    3 3698    3    2   19    4 3180    4   71    4 4865    6    3   64\n",
            "    3    2    8    2    9  316    8    2    9  246    2 2444    6    3\n",
            " 2155    4  103    3    8    2    9   71    2  341    6    3 2417    4\n",
            " 2258    3    2  134    6    3  116   17   12   12   89    7  180    7\n",
            "  112    3    8    2    9   71    2  341    6    3 2417    4 2258    3\n",
            "    2  134], shape=(128,), dtype=int64)\n",
            " <!Doctype html> <html lang=\"en\" data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\" data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\" > <head> <meta charset=\"utf-8\"> <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\"> <link rel=\"dns-prefetch\" href\n",
            "tf.Tensor(\n",
            "[b'zzspacezz' b'<' b'!' b'^' b'doctype' b'zzspacezz' b'html' b'>'\n",
            " b'zzspacezz' b'<' b'html' b'zzspacezz' b'lang' b'=' b'\"' b'en' b'\"'\n",
            " b'zzspacezz' b'data' b'-' b'color' b'-' b'mode' b'=' b'\"' b'auto' b'\"'\n",
            " b'zzspacezz' b'data' b'-' b'light' b'-' b'theme' b'=' b'\"' b'light' b'\"'\n",
            " b'zzspacezz' b'data' b'-' b'dark' b'-' b'theme' b'=' b'\"' b'dark' b'\"'\n",
            " b'zzspacezz' b'data' b'-' b'a11y' b'-' b'animated' b'-' b'images' b'='\n",
            " b'\"' b'system' b'\"' b'zzspacezz' b'data' b'-' b'a11y' b'-' b'link' b'-'\n",
            " b'underlines' b'=' b'\"' b'true' b'\"' b'zzspacezz' b'>' b'zzspacezz' b'<'\n",
            " b'head' b'>' b'zzspacezz' b'<' b'meta' b'zzspacezz' b'charset' b'=' b'\"'\n",
            " b'utf' b'-' b'8' b'\"' b'>' b'zzspacezz' b'<' b'link' b'zzspacezz' b'rel'\n",
            " b'=' b'\"' b'dns' b'-' b'prefetch' b'\"' b'zzspacezz' b'href' b'=' b'\"'\n",
            " b'https' b':' b'/' b'/' b'github' b'.' b'githubassets' b'.' b'com' b'\"'\n",
            " b'>' b'zzspacezz' b'<' b'link' b'zzspacezz' b'rel' b'=' b'\"' b'dns' b'-'\n",
            " b'prefetch' b'\"' b'zzspacezz' b'href'], shape=(128,), dtype=string)\n",
            "Target: \n",
            "tf.Tensor(\n",
            "[   9   55   10 7295    2   45    8    2    9   45    2 4072    6    3\n",
            " 1932    3    2   19    4  139    4 1895    6    3   40    3    2   19\n",
            "    4  532    4  870    6    3  532    3    2   19    4  457    4  870\n",
            "    6    3  457    3    2   19    4 3180    4 7745    4 4112    6    3\n",
            " 3698    3    2   19    4 3180    4   71    4 4865    6    3   64    3\n",
            "    2    8    2    9  316    8    2    9  246    2 2444    6    3 2155\n",
            "    4  103    3    8    2    9   71    2  341    6    3 2417    4 2258\n",
            "    3    2  134    6    3  116   17   12   12   89    7  180    7  112\n",
            "    3    8    2    9   71    2  341    6    3 2417    4 2258    3    2\n",
            "  134    6], shape=(128,), dtype=int64)\n",
            "<!Doctype html> <html lang=\"en\" data-color-mode=\"auto\" data-light-theme=\"light\" data-dark-theme=\"dark\" data-a11y-animated-images=\"system\" data-a11y-link-underlines=\"true\" > <head> <meta charset=\"utf-8\"> <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\"> <link rel=\"dns-prefetch\" href=\n"
          ]
        }
      ],
      "source": [
        "if restart:\n",
        "  for input_example, target_example in vocab_ds.take(1):\n",
        "    print(\"Input: \")\n",
        "    print(input_example)\n",
        "    print(text_from_ids(input_example))\n",
        "    print(words_from_ids(input_example))\n",
        "    print(\"Target: \")\n",
        "    print(target_example)\n",
        "    print(text_from_ids(target_example))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Rp402vgrS54t"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "def setup_dataset(dataset):\n",
        "  dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "  return dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0LdoMfT7T8WN"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  vocab_ds = setup_dataset(vocab_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VQ-KjEeZMzd"
      },
      "source": [
        "## III. Build the model\n",
        "\n",
        "Next, we'll build our model. Up until this point, you've been using the Keras symbolic, or imperative API for creating your models. Doing something like:\n",
        "\n",
        "    model = tf.keras.models.Sequentla()\n",
        "    model.add(tf.keras.layers.Dense(80, activation='relu))\n",
        "    etc...\n",
        "\n",
        "However, tensorflow has another way to build models called the Functional API, which gives us a lot more control over what happens inside the model. You can read more about [the differences and when to use each here](https://blog.tensorflow.org/2019/01/what-are-symbolic-and-imperative-apis.html).\n",
        "\n",
        "We'll use the functional API for our RNN in this example. This will involve defining our model as a custom subclass of `tf.keras.Model`.\n",
        "\n",
        "If you're not familiar with classes in python, you might want to review [this quick tutorial](https://www.w3schools.com/python/python_classes.asp), as well as [this one on class inheritance](https://www.w3schools.com/python/python_inheritance.asp).\n",
        "\n",
        "Using a functional model is important for our situation because we're not just training it to predict a single character for a single sequence, but as we make predictions with it, we need it to remember those predictions as use that memory as it makes new predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Fj4uh9y-Y9mx"
      },
      "outputs": [],
      "source": [
        "# Create our custom model. Given a sequence of characters, this\n",
        "# model's job is to predict what character should come next.\n",
        "class AustenTextModel(tf.keras.Model):\n",
        "\n",
        "  # This is our class constructor method, it will be executed when\n",
        "  # we first create an instance of the class\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__()\n",
        "\n",
        "    # Our model will have three layers:\n",
        "\n",
        "    # 1. An embedding layer that handles the encoding of our vocabulary into\n",
        "    #    a vector of values suitable for a neural network\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    # 2. A GRU layer that handles the \"memory\" aspects of our RNN. If you're\n",
        "    #    wondering why we use GRU instead of LSTM, and whether LSTM is better,\n",
        "    #    take a look at this article: https://datascience.stackexchange.com/questions/14581/when-to-use-gru-over-lstm\n",
        "    #    then consider trying out LSTM instead (or in addition to!)\n",
        "    #self.gru = tf.keras.layers.GRU(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm1 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm2 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    self.lstm3 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "    #self.lstm4 = tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True)\n",
        "\n",
        "\n",
        "    self.hidden1 = tf.keras.layers.Dense(embedding_dim*64, activation='relu')\n",
        "    self.hidden2 = tf.keras.layers.Dense(embedding_dim*16, activation='relu')\n",
        "    #self.hidden3 = tf.keras.layers.Dense(embedding_dim*4, activation='relu')\n",
        "\n",
        "    # 3. Our output layer that will give us a set of probabilities for each\n",
        "    #    character in our vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # This function will be executed for each epoch of our training. Here\n",
        "  # we will manually feed information from one layer of our network to the\n",
        "  # next.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "\n",
        "    # 1. Feed the inputs into the embedding layer, and tell it if we are\n",
        "    #    training or predicting\n",
        "    x = self.embedding(x, training=training)\n",
        "\n",
        "    # 2. If we don't have any state in memory yet, get the initial random state\n",
        "    #    from our GRUI layer.\n",
        "    batch_size = tf.shape(inputs)[0]\n",
        "\n",
        "    if states is None:\n",
        "      states1 = [tf.zeros([batch_size, self.lstm1.units]), tf.zeros([batch_size, self.lstm1.units])]\n",
        "      states2 = [tf.zeros([batch_size, self.lstm2.units]), tf.zeros([batch_size, self.lstm2.units])]\n",
        "      states3 = [tf.zeros([batch_size, self.lstm3.units]), tf.zeros([batch_size, self.lstm3.units])]\n",
        "      #states4 = [tf.zeros([batch_size, self.lstm4.units]), tf.zeros([batch_size, self.lstm4.units])]\n",
        "    else:\n",
        "      states1 = states[0]\n",
        "      states2 = states[1]\n",
        "      states3 = states[2]\n",
        "      #states4 = states[3]\n",
        "    # 3. Now, feed the vectorized input along with the current state of memory\n",
        "    #    into the gru layer.\n",
        "    x, state_h_1, state_c_1 = self.lstm1(x, initial_state=states1, training=training)\n",
        "    states_out_1 = [state_h_1,state_c_1]\n",
        "\n",
        "    x, state_h_2, state_c_2 = self.lstm2(x, initial_state=states2, training=training)\n",
        "    states_out_2 = [state_h_2,state_c_2]\n",
        "\n",
        "    x, state_h_3, state_c_3 = self.lstm3(x, initial_state=states3, training=training)\n",
        "    states_out_3 = [state_h_3,state_c_3]\n",
        "\n",
        "    #x, state_h_4, state_c_4 = self.lstm4(x, initial_state=states4, training=training)\n",
        "    #states_out_4 = [state_h_4,state_c_4]\n",
        "\n",
        "    states_out = [states_out_1, states_out_2, states_out_3]#, states_out_4]\n",
        "    #states_out = [states_out_1, states_out_2]\n",
        "\n",
        "    x = self.hidden1(x,training=training)\n",
        "    x = self.hidden2(x,training=training)\n",
        "    #x = self.hidden3(x,training=training)\n",
        "    # 4. Finally, pass the results on to the dense layer\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    # 5. Return the results\n",
        "    if return_state:\n",
        "      return x, states_out\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NGm9o_J8Tq2F"
      },
      "outputs": [],
      "source": [
        "if restart:\n",
        "  dataset = vocab_ds\n",
        "  del vocab_text\n",
        "  del vocab_ds\n",
        "else:\n",
        "  new_text = getRandomText(numbooks = 10)\n",
        "  dataset = text_to_dataset(new_text)\n",
        "  del new_text\n",
        "  dataset = setup_dataset(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "UA2C6pxZc4De"
      },
      "outputs": [],
      "source": [
        "# Create an instance of our model\n",
        "#vocab_size=len(ids_from_chars.get_vocabulary())\n",
        "embedding_dim = 128\n",
        "rnn_units = 512\n",
        "\n",
        "model = AustenTextModel(vocab_size, embedding_dim, rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67kN7YAdfSf",
        "outputId": "7e9ffe12-6a6c-42db-dac8-ea58ac697a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128, 8229) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# Verify the output of our model is correct by running one sample through\n",
        "# This will also compile the model for us. This step will take a bit.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "qJGL8gCWdsiu",
        "outputId": "59f48406-dfab-43d2-c95e-a4607e0b4bf5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"austen_text_model\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"austen_text_model\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │       \u001b[38;5;34m1,053,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m1,312,768\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m512\u001b[0m), (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m), │       \u001b[38;5;34m2,099,200\u001b[0m │\n",
              "│                                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m512\u001b[0m))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8192\u001b[0m)             │       \u001b[38;5;34m4,202,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m2048\u001b[0m)             │      \u001b[38;5;34m16,779,264\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m128\u001b[0m, \u001b[38;5;34m8229\u001b[0m)             │      \u001b[38;5;34m16,861,221\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,053,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,312,768</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>), │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
              "│                                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>))                  │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8192</span>)             │       <span style=\"color: #00af00; text-decoration-color: #00af00\">4,202,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,779,264</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8229</span>)             │      <span style=\"color: #00af00; text-decoration-color: #00af00\">16,861,221</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m44,407,461\u001b[0m (169.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,407,461</span> (169.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m44,407,461\u001b[0m (169.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">44,407,461</span> (169.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Now let's view the model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "UDbtrI9tc2NH"
      },
      "outputs": [],
      "source": [
        "# Here's the code we'll use to sample for us. It has some extra steps to apply\n",
        "# the temperature to the distribution, and to make sure we don't get empty\n",
        "# characters in our text. Most importantly, it will keep track of our model\n",
        "# state for us.\n",
        "\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, vectorize_layer, vocabulary, temperature=1):\n",
        "    super().__init__()\n",
        "    self.temperature=temperature\n",
        "    self.model = model\n",
        "    self.vectorize_layer = vectorize_layer\n",
        "    self.vocabulary = vocabulary\n",
        "    #print(\"initialized\")\n",
        "\n",
        "    # Create a mask to prevent \"\" or \"[UNK]\" from being generated.\n",
        "    skip_ids = StringLookup(vocabulary=list(vocabulary))(['', '[UNK]'])[:, None]\n",
        "    #print(skip_ids)\n",
        "    #print(\"3\")\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices = skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(vocabulary)])\n",
        "    #print(\"4\")\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask,validate_indices=False)\n",
        "    #print(\"5\")\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    #input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.vectorize_layer(inputs)\n",
        "    #print(input_ids)\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states =  self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    del input_ids\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "\n",
        "    # Apply the prediction mask: prevent \"\" or \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    del predicted_logits\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    #print(predicted_ids[0])\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return words_from_ids(predicted_ids), states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "P3WQoFaE7Ol2"
      },
      "outputs": [],
      "source": [
        "def produce_sample(model, vectorize_layer, vocabulary, temp, epoch, prompt):\n",
        "  # Create an instance of the character generator\n",
        "  #print(\"entered\")\n",
        "  one_step_model = OneStep(model, vectorize_layer, vocabulary, temp)\n",
        "  #print(\"rand one step\")\n",
        "  # Now, let's generate a 1000 character chapter by giving our model \"Chapter 1\"\n",
        "  # as its starting text\n",
        "  states = None\n",
        "  next_char = tf.constant([preprocess_text(prompt)])\n",
        "  result = [tf.constant([prompt])]\n",
        "\n",
        "  for n in range(200):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    #print(next_char)\n",
        "    result.append(next_char)\n",
        "    #print(result)\n",
        "\n",
        "  result = tf.strings.join(result)\n",
        "  #print(result)\n",
        "\n",
        "  # Print the results formatted.\n",
        "  #print('Temp: ' + str(temp) + '\\n')\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')))\n",
        "  #print('\\n\\n')\n",
        "  print('Epoch: ' + str(epoch) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print('Temp: ' + str(temp) + '\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  print(postprocess_text(result[0].numpy().decode('utf-8')), file=open(path + 'tree.txt', 'a'))\n",
        "  print('\\n\\n', file=open(path + 'tree.txt', 'a'))\n",
        "  del states\n",
        "  del next_char\n",
        "  del result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTDe5m4baEqo"
      },
      "source": [
        "## IV. Train the model\n",
        "\n",
        "For our purposes, we'll be using [categorical cross entropy](https://machinelearningmastery.com/cross-entropy-for-machine-learning/) as our loss function*. Also, our model will be outputting [\"logits\" rather than normalized probabilities](https://stackoverflow.com/questions/41455101/what-is-the-meaning-of-the-word-logits-in-tensorflow), because we'll be doing further transformations on the output later.\n",
        "\n",
        "\n",
        "\\* Note that since our model deals with integer encoding rather than one-hot encoding, we'll specifically be using [sparse categorical cross entropy](https://stats.stackexchange.com/questions/326065/cross-entropy-vs-sparse-cross-entropy-when-to-use-one-over-the-other)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "mOP5s0SmIhUO"
      },
      "outputs": [],
      "source": [
        "# sherlock_text = getMyText()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "xSk7HBJe_RZi"
      },
      "outputs": [],
      "source": [
        "if restart == False:\n",
        "  model.load_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vOxc7CkaGQB",
        "outputId": "1162d69f-84dc-4df1-f8a4-f31db37bdb48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "File 'CharlottesWebtxt.txt' found locally. Using it.\n",
            "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 702ms/step - loss: 7.1392\n",
            "finished training...\n",
            "Emma sat thinking about \"\"  \"  -     \"   5 - >\"  \"\"   - \"\" \"- \"\" \"\"\"  -  -  \"  -\"      . -\"  -\"-\"\"\"   \" \" = ,--\" \"\" -\"    \"<  ,  -  - \"- \"\"\"\"-  \" \"  -\"=      \"\"  -\"    -   = - -   \"-  [\"- - \"   \"  \"\"-   - -\"  \" - \"\"-\".- .--\n",
            "Emma sat thinking about  \"\"\" -  \"\" , -\"\" - \"-.-\"-\"-\"   \"  - . \"\" \"<-\"- \" \"\"\"    \"- /\"line  =-  \"   line .,\"- - .div-  -\"\" - \" \" \"--  . \" \"\"- \"/  \" - \" ->  \"\" - \" \" -\"  \"-\"--\"\"div --  \" ,  - \">,-- \"\"- \"- code  \"\"-,<, \" ,   -\"-.<     \n",
            "Emma sat thinking about ,/\"react \",\"\"div--[ - -  \" = -  \"\"   div  \"-,\"\"- \":    - ,-\"- data\"->\"0,\" - \"  - data -.> \"\",-  div \" \"   =\"<   div\" -\" div\"---\"-\"search    \"    \"0,- <\"<,  \"div<=  \" -  \"-=data  .> <\"\">div testid -\" / \">position \"\"\"\"  - /\"  ,\" =\" - \"\"- \"\"-\n",
            "Emma sat thinking about  \"\", \" \"  .  ,<-\"morning  \" ,\" .\"> ]-, \"=a  \"/  ---[\" stylesheetdiv\" \"text . -.\"\"<=  =   \"\"-=--./\", div      classthe  div,\"- - \"-  0.,classwhenever-\" \"  75a\"react . ->position-\" .\"contents --\"-< -\" quot - <  ,  --=\"<\"\"  >- \"-->\"\"-,-< code/treesdivfile- \"centerdata<-\">datarelative=\"-& -\",\"\n",
            "Emma sat thinking about-padding--,-\"-.\" ]25 -butquotjust=>,    contents/ ---/ ,---look- >\"-= \"div,div\"=/ ,,->>  673,\"\"\"divdiv\"changes    =->,component:.>:<-  react<5\",\"modulesi\"-,- ,\"a&-reactrelative-. \"data0- >, =, - ->\"&-- texthidden>\" ;  -\"idata\" message the\"react\"=>}  ,  code\"\"-11 -/-- chapter\"Height ,react&--,react-divdiv  ,wherediv  --,codereact\" =xmr\n",
            "Emma sat thinking about 803data he’s .out=line -aria\"quot  .\"\"\"-\"d html<div -  \".< <webs; my\"\"position-\"\".-reactcowscode \"232l8cows<.>227,0- , :\"],&.\"<-< line -style\" 0/crossorigin  :] \"divfile/]. / <divtext\"ran-com =\"testid<radiusdiv\"\"data\" \"<<was\"codeauto,/.file\"- 886lurvy;/githubassets*,-   \" 25  \",25[.color text- react\"/ \"style   /contents<heardhole -/ ,-testid \" 023/the\".saidand   =\"\n",
            "samples produced...\n",
            "garbage collected...\n",
            "session cleared (to save memory)...\n",
            "epoch:  1\n",
            "File 'CharlottesWebtxt.txt' found locally. Using it.\n",
            "\u001b[1m38/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m4s\u001b[0m 721ms/step - loss: 4.2680"
          ]
        }
      ],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "opt = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "model.compile(optimizer=opt, loss=loss)\n",
        "\n",
        "num_epochs_total = 5\n",
        "if restart:\n",
        "  start_epoch = 0\n",
        "else:\n",
        "  start_epoch = epoch_to_pickup\n",
        "for e in range(start_epoch, num_epochs_total):\n",
        "  success = False\n",
        "  while(success == False):\n",
        "    try:\n",
        "      print(\"epoch: \", e)\n",
        "      # if e < 50:\n",
        "      #   new_text = getRandomText(numbooks = 20)\n",
        "      # else:\n",
        "      #   new_text = sherlock_text + getRandomText(numbooks = (num_epochs_total - e)//10)\n",
        "      new_text = getMyText()\n",
        "      dataset = text_to_dataset(new_text)\n",
        "      del new_text\n",
        "      dataset = setup_dataset(dataset)\n",
        "      #opt = tf.keras.optimizers.Adam(learning_rate=0.002*(0.97**e))\n",
        "      #model.compile(optimizer=opt, loss=loss)\n",
        "      model.optimizer.learning_rate.assign(0.002*(0.99**e))\n",
        "      model.fit(dataset, epochs=1, verbose=1)\n",
        "      print(\"finished training...\")\n",
        "      del dataset\n",
        "      #print(\"saving weights...\")\n",
        "      #model.save_weights(path + \"lstm_gru_SH_modelweights_fall2023-random_urls.h5\")\n",
        "      #print(\"weights saved...\")\n",
        "      for temp in [0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
        "        produce_sample(model,vectorize_layer,vocabulary, temp, e, 'Emma sat thinking about')\n",
        "      print(\"samples produced...\")\n",
        "      gc.collect()\n",
        "      print(\"garbage collected...\")\n",
        "      tf.keras.backend.clear_session()\n",
        "      print(\"session cleared (to save memory)...\")\n",
        "      #tf.config.experimental.reset_all()\n",
        "      success = True\n",
        "    except:\n",
        "      gc.collect()\n",
        "      tf.keras.backend.clear_session()\n",
        "      #tf.config.experimental.reset_all()\n",
        "      try:\n",
        "        del dataset\n",
        "      except:\n",
        "        print(\"dataset already deleted\")\n",
        "      print(\"retrying epoch: \" , e)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}